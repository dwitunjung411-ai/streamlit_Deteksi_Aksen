# -*- coding: utf-8 -*-
"""stream_aksendeteksi

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nyecUSqB51UGioORtV_BILcGHoDTKZli
"""
import streamlit as st
import numpy as np
import librosa
import soundfile as sf
import tensorflow as tf
import tempfile
import os
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from tensorflow.keras import layers
from tensorflow.keras.models import Model
import io

# Set page config
st.set_page_config(
    page_title="Aksen Bahasa Indonesia - Few-Shot Learning",
    page_icon="üé§",
    layout="wide"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E3A8A;
        text-align: center;
        margin-bottom: 2rem;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #3B82F6;
        margin-bottom: 1rem;
    }
    .info-box {
        background-color: #F0F8FF;
        padding: 20px;
        border-radius: 10px;
        border-left: 5px solid #3B82F6;
        margin: 10px 0;
    }
    .prediction-box {
        background-color: #E8F5E9;
        padding: 20px;
        border-radius: 10px;
        border-left: 5px solid #4CAF50;
        margin: 10px 0;
    }
    .warning-box {
        background-color: #FFF3E0;
        padding: 15px;
        border-radius: 10px;
        border-left: 5px solid #FF9800;
        margin: 10px 0;
    }
    .stButton button {
        background-color: #3B82F6;
        color: white;
        font-weight: bold;
        border-radius: 5px;
        padding: 10px 20px;
    }
</style>
""", unsafe_allow_html=True)

# Title
st.markdown('<h1 class="main-header">üé§ Klasifikasi Aksen Bahasa Indonesia</h1>', unsafe_allow_html=True)
st.markdown("### Menggunakan Model Few-Shot Learning")

# Sidebar
with st.sidebar:
    st.image("https://img.icons8.com/color/96/000000/microphone.png", width=80)
    st.markdown("### ‚öôÔ∏è Pengaturan")

    # Model parameters
    st.markdown("#### Parameter Model")
    n_way = st.slider("Jumlah Kelas (n-way)", min_value=2, max_value=10, value=5)
    k_shot = st.slider("Contoh per Kelas (k-shot)", min_value=1, max_value=10, value=3)
    q_query = st.slider("Query per Kelas (q-query)", min_value=1, max_value=10, value=2)

    # Audio parameters
    st.markdown("#### Parameter Audio")
    target_sr = st.selectbox("Sample Rate Target", [22050, 44100, 16000], index=0)
    n_mfcc = st.slider("Jumlah MFCC", min_value=13, max_value=40, value=40)

    st.markdown("---")
    st.markdown("### üìä Informasi Dataset")
    st.info("""
    Dataset terdiri dari 5 aksen:
    - Betawi
    - Jawa Timur
    - Jawa Tengah
    - Sunda
    - YogyaKarta
    """)

# Load LabelEncoders (dummy data for demonstration)
@st.cache_resource
def load_encoders():
    """Load label encoders for the model"""
    classes = ['Betawi', 'Jawa_Timur', 'Jawa_Tengah', 'Sunda', 'YogyaKarta']

    le_y = LabelEncoder()
    le_y.fit(classes)

    le_gender = LabelEncoder()
    le_gender.fit(['laki - laki', 'perempuan'])

    le_provinsi = LabelEncoder()
    le_provinsi.fit(['DKI Jakarta', 'Jawa Barat', 'Jawa Tengah', 'Jawa Timur', 'D.I YogyaKarta'])

    return le_y, le_gender, le_provinsi

# Define embedding model (same as in notebook)
def build_embedding_model(input_shape):
    model = tf.keras.Sequential([
        layers.Conv2D(128, (3,3), activation='relu', input_shape=input_shape),
        layers.MaxPooling2D((2,2)),
        layers.GlobalAveragePooling2D(),
        layers.Dense(256, activation='relu'),
        layers.Dropout(0.3),
        layers.Dense(128, activation='relu')
    ])
    return model

# Prototypical Network (same as in notebook)
class PrototypicalNetwork(Model):
    def __init__(self, embedding_model):
        super(PrototypicalNetwork, self).__init__()
        self.embedding = embedding_model

    def call(self, support_set, query_set, support_labels, n_way):
        # Calculate embeddings
        support_embeddings = self.embedding(support_set)
        query_embeddings = self.embedding(query_set)

        # Calculate prototypes per class
        prototypes = []
        for i in range(n_way):
            mask = tf.equal(support_labels, i)
            class_embeddings = tf.boolean_mask(support_embeddings, mask)
            prototype = tf.reduce_mean(class_embeddings, axis=0)
            prototypes.append(prototype)
        prototypes = tf.stack(prototypes)

        # Calculate Euclidean distance between query and prototypes
        distances = []
        for q in query_embeddings:
            dist = tf.norm(prototypes - q, axis=1)
            distances.append(dist)
        distances = tf.stack(distances)

        # Convert distances to probabilities (softmax over negative distances)
        logits = -distances
        return logits

# Audio preprocessing function
def preprocess_audio(audio_file, target_sr=22050, n_mfcc=40, max_len=174):
    """Preprocess audio file and extract MFCC features"""
    try:
        # Load audio
        audio_data, sr = sf.read(audio_file)

        # Convert to mono if stereo
        if len(audio_data.shape) > 1:
            audio_data = np.mean(audio_data, axis=1)

        # Resample if needed
        if sr != target_sr:
            audio_data = librosa.resample(audio_data, orig_sr=sr, target_sr=target_sr)
            sr = target_sr

        # Normalize amplitude
        audio_data = librosa.util.normalize(audio_data)

        # Extract MFCC
        mfcc = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=n_mfcc, n_fft=2048, hop_length=512)

        # Delta and Delta-Delta
        delta = librosa.feature.delta(mfcc)
        delta2 = librosa.feature.delta(mfcc, order=2)

        # Padding or truncating for uniform length
        if mfcc.shape[1] < max_len:
            pad_width = max_len - mfcc.shape[1]
            mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')
            delta = np.pad(delta, ((0, 0), (0, pad_width)), mode='constant')
            delta2 = np.pad(delta2, ((0, 0), (0, pad_width)), mode='constant')
        else:
            mfcc = mfcc[:, :max_len]
            delta = delta[:, :max_len]
            delta2 = delta2[:, :max_len]

        # Stack into 3 channels
        features = np.stack([mfcc, delta, delta2], axis=-1)

        return features, audio_data, sr

    except Exception as e:
        st.error(f"Error processing audio: {str(e)}")
        return None, None, None

# Create episode function
def create_episode(data, labels, n_way=5, k_shot=5, q_query=5):
    """Create few-shot learning episode"""
    unique_labels = np.unique(labels)
    selected_labels = np.random.choice(unique_labels, n_way, replace=False)

    support_set = []
    query_set = []
    support_labels = []
    query_labels = []

    for label in selected_labels:
        indices = np.where(labels == label)[0]
        sampled_indices = np.random.choice(indices, k_shot + q_query, replace=False)

        support_indices = sampled_indices[:k_shot]
        query_indices = sampled_indices[k_shot:]

        support_set.append(data[support_indices])
        query_set.append(data[query_indices])

        support_labels.extend([label] * k_shot)
        query_labels.extend([label] * q_query)

    support_set = np.vstack(support_set)
    query_set = np.vstack(query_set)
    support_labels = np.array(support_labels)
    query_labels = np.array(query_labels)

    return support_set, query_set, support_labels, query_labels

# Load encoders
le_y, le_gender, le_provinsi = load_encoders()

# Main app
def main():
    # Create tabs
    tab1, tab2, tab3 = st.tabs(["üé§ Prediksi Aksen", "üìä Analisis Audio", "‚ÑπÔ∏è Tentang Model"])

    with tab1:
        st.markdown('<h2 class="sub-header">Prediksi Aksen dari Audio</h2>', unsafe_allow_html=True)

        col1, col2 = st.columns([2, 1])

        with col1:
            # Audio input options
            input_option = st.radio(
                "Pilih sumber audio:",
                ["Upload File", "Rekam Langsung"],
                horizontal=True
            )

            audio_file = None

            if input_option == "Upload File":
                uploaded_file = st.file_uploader(
                    "Upload file audio (format: .wav, .mp3)",
                    type=['wav', 'mp3', 'm4a']
                )

                if uploaded_file is not None:
                    # Save uploaded file temporarily
                    with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:
                        tmp_file.write(uploaded_file.read())
                        audio_file = tmp_file.name

                    # Display audio player
                    st.audio(uploaded_file, format='audio/wav')

            else:  # Record directly
                st.warning("Fitur rekaman langsung memerlukan setup tambahan. Untuk demo, silakan upload file audio.")
                # Note: For actual recording, you would need additional libraries like streamlit-webrtc

        with col2:
            st.markdown("### Informasi Tambahan")
            usia = st.number_input("Usia", min_value=5, max_value=100, value=30)
            gender = st.selectbox("Jenis Kelamin", ['laki - laki', 'perempuan'])
            provinsi = st.selectbox("Provinsi Asal", ['DKI Jakarta', 'Jawa Barat', 'Jawa Tengah', 'Jawa Timur', 'D.I YogyaKarta'])

            # Metadata encoding
            usia_scaled = (usia - 30) / 10  # Simple scaling

            gender_encoded = le_gender.transform([gender])[0]
            provinsi_encoded = le_provinsi.transform([provinsi])[0]

            # One-hot encode categorical features
            cat_encoded = np.zeros(len(le_gender.classes_) + len(le_provinsi.classes_))
            cat_encoded[gender_encoded] = 1
            cat_encoded[len(le_gender.classes_) + provinsi_encoded] = 1

            metadata = np.hstack([[usia_scaled], cat_encoded]).astype(np.float32)

        # Prediction button
        if st.button("üîç Analisis Aksen", type="primary") and audio_file:
            with st.spinner("Memproses audio dan melakukan prediksi..."):
                try:
                    # Preprocess audio
                    features, audio_data, sr = preprocess_audio(
                        audio_file,
                        target_sr=target_sr,
                        n_mfcc=n_mfcc
                    )

                    if features is not None:
                        # Create broadcasted metadata
                        features_shape = features.shape
                        metadata_broadcast = np.repeat(
                            metadata[np.newaxis, np.newaxis, np.newaxis, :],
                            features_shape[1], axis=1
                        )
                        metadata_broadcast = np.repeat(
                            metadata_broadcast,
                            features_shape[2], axis=2
                        )

                        # Combine features with metadata
                        final_features = np.concatenate(
                            [features, metadata_broadcast],
                            axis=-1
                        ).astype(np.float32)

                        # For demo purposes, we'll create a mock episode
                        # In production, you would load a pre-trained model
                        st.markdown('<div class="info-box">', unsafe_allow_html=True)
                        st.write("**Status:** Memuat model dan membuat episode few-shot...")

                        # Simulate model prediction
                        np.random.seed(42)
                        probabilities = np.random.dirichlet(np.ones(5), size=1)[0]
                        predicted_class_idx = np.argmax(probabilities)
                        predicted_accent = le_y.inverse_transform([predicted_class_idx])[0]

                        # Display results
                        st.markdown('</div>', unsafe_allow_html=True)
                        st.markdown('<div class="prediction-box">', unsafe_allow_html=True)

                        col_pred1, col_pred2 = st.columns(2)

                        with col_pred1:
                            st.metric("üéØ Aksen Terprediksi", predicted_accent)
                            st.metric("üìä Confidence", f"{probabilities[predicted_class_idx]*100:.1f}%")

                        with col_pred2:
                            # Show all probabilities
                            st.write("**Probabilitas semua kelas:**")
                            for idx, prob in enumerate(probabilities):
                                accent_name = le_y.inverse_transform([idx])[0]
                                st.progress(float(prob), text=f"{accent_name}: {prob*100:.1f}%")

                        st.markdown('</div>', unsafe_allow_html=True)

                        # Clean up temp file
                        os.unlink(audio_file)

                except Exception as e:
                    st.error(f"Terjadi error: {str(e)}")
                    if audio_file and os.path.exists(audio_file):
                        os.unlink(audio_file)

        elif st.button("üîç Analisis Aksen") and not audio_file:
            st.warning("Silakan upload atau rekam audio terlebih dahulu!")

    with tab2:
        st.markdown('<h2 class="sub-header">Analisis Fitur Audio</h2>', unsafe_allow_html=True)

        if 'audio_data' in locals() and audio_data is not None and 'sr' in locals():
            col1, col2 = st.columns(2)

            with col1:
                # Waveform plot
                fig, ax = plt.subplots(figsize=(10, 4))
                librosa.display.waveshow(audio_data, sr=sr, ax=ax)
                ax.set_title('Waveform Audio')
                ax.set_xlabel('Time (s)')
                ax.set_ylabel('Amplitude')
                st.pyplot(fig)

            with col2:
                # Spectrogram
                fig, ax = plt.subplots(figsize=(10, 4))
                D = librosa.amplitude_to_db(np.abs(librosa.stft(audio_data)), ref=np.max)
                img = librosa.display.specshow(D, y_axis='log', x_axis='time', sr=sr, ax=ax)
                ax.set_title('Spectrogram')
                fig.colorbar(img, ax=ax, format="%+2.0f dB")
                st.pyplot(fig)

            # MFCC features
            if 'features' in locals() and features is not None:
                fig, axes = plt.subplots(3, 1, figsize=(12, 8))
                titles = ['MFCC', 'Delta MFCC', 'Delta-Delta MFCC']
    
                for i, (ax, title) in enumerate(zip(axes, titles)):
                    # Ambil fitur dan pastikan hanya satu dimensi terakhir yang dipilih
                    feature_plot = features[0, :, i]  # Ambil fitur untuk dimensi ke-3
                    
                    # Plot fitur menggunakan imshow, karena sekarang sudah 2D
                    img = ax.imshow(feature_plot.T, aspect='auto', origin='lower', cmap='viridi ')
                    ax.set_title(title)
                    ax.set_xlabel('Frame')
                    ax.set_ylabel('MFCC Coefficient')
                    fig.colorbar(img, ax=ax)
    
                plt.tight_layout()
                st.pyplot(fig)


        else:
            st.info("Upload audio di tab 'Prediksi Aksen' untuk melihat analisis fitur.")

    with tab3:
        st.markdown('<h2 class="sub-header">Tentang Model Few-Shot Learning</h2>', unsafe_allow_html=True)

        st.markdown("""
        ### üìö Model Architecture

        Model ini menggunakan **Prototypical Networks** untuk few-shot learning dengan arsitektur berikut:

        1. **Embedding Network:**
           - Conv2D (128 filters, 3x3 kernel)
           - MaxPooling2D (2x2)
           - GlobalAveragePooling2D
           - Dense (256 units) + Dropout (0.3)
           - Dense (128 units) ‚Üí Embedding output

        2. **Prototypical Network:**
           - Menghitung prototype untuk setiap kelas dari support set
           - Mengukur jarak Euclidean antara query dan prototypes
           - Menggunakan negative distance sebagai logits untuk klasifikasi

        ### üéØ Few-Shot Learning Setup

        - **n-way:** Jumlah kelas dalam setiap episode
        - **k-shot:** Jumlah contoh per kelas di support set
        - **q-query:** Jumlah query per kelas untuk evaluasi

        ### üîä Ekstraksi Fitur Audio

        Fitur audio diekstraksi menggunakan:
        - **MFCC (Mel-Frequency Cepstral Coefficients):** 40 koefisien
        - **Delta MFCC:** Perubahan MFCC terhadap waktu
        - **Delta-Delta MFCC:** Perubahan delta terhadap waktu
        - **Metadata:** Usia, gender, dan provinsi digabungkan sebagai fitur tambahan

        ### üìä Dataset

        Dataset terdiri dari 300 sampel audio dengan distribusi:
        - 5 aksen berbeda (60 sampel per aksen)
        - Variasi usia, gender, dan provinsi
        - Sample rate: 22.05 kHz

        ### ‚ö†Ô∏è Catatan

        Aplikasi ini adalah **demo** dengan model simulasi. Dalam implementasi produksi:
        1. Model perlu dilatih dengan dataset yang komprehensif
        2. Validasi cross-episode diperlukan
        3. Hyperparameter tuning optimal
        4. Deployment dengan model yang sudah dilatih
        """)

        # Show model parameters
        st.markdown("### ‚öôÔ∏è Parameter Model Saat Ini")
        params = {
            "n-way": n_way,
            "k-shot": k_shot,
            "q-query": q_query,
            "Sample Rate": f"{target_sr} Hz",
            "MFCC Coefficients": n_mfcc,
            "Jumlah Kelas": len(le_y.classes_)
        }

        for key, value in params.items():
            st.write(f"**{key}:** {value}")

# Run the app
if __name__ == "__main__":
    # Check TensorFlow version
    st.sidebar.markdown("---")
    st.sidebar.markdown("### üîß Informasi Teknis")
    st.sidebar.write(f"TensorFlow: {tf.__version__}")
    st.sidebar.write(f"Librosa: {librosa.__version__}")

    # Warning for demo purposes
    st.sidebar.markdown("""
    <div class="warning-box">
    ‚ö†Ô∏è <strong>Demo Simulasi</strong><br>
    Prediksi menggunakan random sampling untuk demonstrasi.
    Model aktual memerlukan training dengan dataset lengkap.
    </div>
    """, unsafe_allow_html=True)

    main()
